<!doctype html>
<html>

<head>
  <meta charset="utf-8" />
  <title>FPBench</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" type="text/css" href="fpbench.css">


  <style>
    #info {
      padding-bottom: 1.25em;
      margin-bottom: 1.5em;
      border-bottom: 1px solid gray;
    }

    details {
      clear: both;
      margin-bottom: 1.5em;
    }

    summary {
      list-style-type: none;
    }

    time {
      float: right;
    }

    .title {
      color: #198C0E;
    }

    details:has(.abstract) .title {
      cursor: pointer;
    }

    .speaker {
      font-style: italic;
      font-size: 90%;
    }

    .abstract {
      font-size: 80%;
      margin-bottom: 2em;
    }
  </style>
</head>

<body>
  <header>
    <a href='index.html'>
      <img src='img/logo.png' height='150' alt='FPBench Logo' />
      <h1>FPBench</h1>
    </a>
    <p>Numerics community meetings and resources</p>
    <ul>
      <li><a href="index.html">Events</a></li>
      <li><a href="education.html">Education</a></li>
      <li><a href="community.html">Tools</a></li>
      <li><a href="https://groups.google.com/a/fpbench.org/g/fpbench">Mailing List</a></li>
      <li><a href="about.html">About</a></li>
    </ul>
  </header>

  <div class="pitch">
    <a href="https://groups.google.com/a/fpbench.org/g/fpbench">Join us</a> monthly to talk numerics.
  </div>

  <p id='info'>
    FPBench meets for an hour, on Zoom, on the first Thursday of every month at 9am Pacific.<abbr
      title="Except that we skip months for the annual workshop and for the winter holidays."><sup>&dagger;</sup></abbr>
    You can <a href="https://forms.gle/H5aodraRcgcJFq6W8">submit a proposal</a> to present.
  </p>


  <main>

    <!--
    <details>
      <summary>
        <time>TIME</time>
        <div class="title">
           TITLE, use sentence not title case
        </div>
        <div class="speaker">
          TODO, University of TODO
        </div>
      </summary>
      <div class="abstract">
        <p>
          TODO
        </p>
      </div>
    </details>
-->

    <!--
    <details>
      <summary>
        <time>July 2, 2026</time>
        <div class="title">
          Semantics, Operations, and Properties of P3109 Floating-Point Representations in Lean
        </div>
        <div class="speaker">
          Tung-Che Chang, Rutgers University
        </div>
      </summary>
      <div class="abstract">
        <p>
          The upcoming IEEE-P3109 standard for low-precision
          floating-point arithmetic can become the foundation of
          future machine learning hardware and software. Unlike the
          fixed types of IEEE-754, P3109 introduces a parametric
          framework defined by bitwidth, precision, signedness, and
          domain. This flexibility results in a vast combinatorial
          space of formats -- some with as little as one bit of
          precision -- alongside novel features such as stochastic
          rounding and saturation arithmetic. These deviations create
          a unique verification gap that this paper intends to
          address.
        </p>

        <p>
          This paper presents FLoPS, Formalization in Lean of the
          P3109 Standard, which is a comprehensive formal model of
          P3109 in Lean. Our work serves as a rigorous,
          machine-checked specification that facilitates deep analysis
          of the standard. We demonstrate the model's utility by
          verifying foundational properties and analyzing key
          algorithms within the P3109 context. Specifically, we reveal
          that FastTwoSum exhibits a novel property of computing exact
          "overflow error" under saturation using any rounding mode,
          whereas previously established properties of the
          ExtractScalar algorithm fail for formats with one bit of
          precision. This work provides a verified foundation for
          reasoning about P3109 and enables formal verification of
          future numerical software. Our Lean development is open
          source and publicly available.
        </p>

        <p>
          More in our <a href="https://arxiv.org/abs/2602.15965">preprint</a>.
        </p>
      </div>
    </details>
-->

    <!--
    <details>
      <summary>
        <time>June 4, 2026</time>
        <div class="title">
           TITLE, use sentence not title case
        </div>
        <div class="speaker">
          Jeffrey Sarnoff, IEEE
        </div>
      </summary>
      <div class="abstract">
        <p>
          TODO
        </p>
      </div>
    </details>
-->

    <!--
    <details>
      <summary>
        <time>May 5, 2026</time>
        <div class="title">
           TITLE, use sentence not title case
        </div>
        <div class="speaker">
          Mantas Mikaitis, University of Leeds
        </div>
      </summary>
      <div class="abstract">
        <p>
          TODO
        </p>
      </div>
    </details>
-->

    <!--
    <details>
      <summary>
        <time>Apr 4, 2026</time>
        <div class="title">
          Cost of soundness in mixed-precision tuning
        </div>
        <div class="speaker">
          Anastasia Isychev, TU Wien
        </div>
      </summary>
      <div class="abstract">
        <p>
          Numerical code is often executed repetitively and on hardware with
          limited resources, which makes it a perfect target for optimizations.
          One of the most effective ways to boost performance—especially in
          terms of runtime—is by reducing the precision of computations.
          However, low precision can introduce significant rounding errors,
          potentially compromising the correctness of results. Mixed-precision
          tuning addresses this trade-off by assigning the lowest possible
          precision to a subset of variables and arithmetic operations in the
          program while ensuring that the overall error remains within
          acceptable bounds. State-of-the-art tools validate the accuracy of
          optimized programs using either sound static analysis or dynamic
          sampling. While sound methods are often considered safer but overly
          conservative, and dynamic methods are more aggressive and potentially
          more effective, the question remains: how do these approaches compare
          in practice?
        </p>
        <p>
          We present the first comprehensive evaluation of existing
          mixed-precision tuning tools for floating-point programs, offering a
          **quantitative comparison** between sound static and (unsound) dynamic
          approaches. We measure the trade-offs between performance gains,
          utilizing optimization potential, and the soundness guarantees on the
          accuracy—what we refer to as the cost of soundness. Our experiments on
          the standard FPBench benchmark suite challenge the common belief that
          dynamic optimizers consistently generate faster programs. In fact, for
          small straight-line numerical programs, we find that sound tools
          enhanced with regime inference match or outperform dynamic ones, while
          providing formal correctness guarantees, albeit at the cost of
          increased optimization time. Standalone sound tools, however, are
          overly conservative, especially when accuracy constraints are tight;
          whereas dynamic tools are consistently effective for different
          targets, but exceed the maximum allowed error by up to 9 orders of
          magnitude.
        </p>
        <p>
          More in our <a href="https://aisychev.github.io/papers/oopsla25-cos.pdf">OOPSLA 2025 paper</a>.
        </p>
      </div>
    </details>
-->

    <details>
      <summary>
        <time>Mar 5, 2026</time>
        <div class="title">
           Challenges and opportunities in large-scale linear systems
        </div>
        <div class="speaker">
          Ichitaro Yamazaki, Sandia National Laboratories
        </div>
      </summary>
      <div class="abstract">
        <p>
          Linear solvers often play paramount roles in large-sale
          scientific and engineering simulations. In order to maintain
          the quality of our software's functionalities and
          performance over various hardwares, our solvers are
          implemented within the open-source software framework,
          called Trilinos. However, it is still a challenge to
          maintain the robust implementation of the parallel solvers,
          and we often spend significant amount of time maintaining
          our solvers on each new hardware. Furthermore, in order to
          effectively utilize the current large-scale computers, many
          techniques have been proposed (communication
          avoiding/hiding, asynchronous communication,
          mixed-precision, random sketching, etc.), which often pose
          further challenges to maintain the reliability of our
          solvers. In this talk, we discuss some of these techniques
          integrated into our solvers, and associated challenges. The
          hope is to see if there are any potential collaboration in
          order to enhance the reliabilities of our solver
          implementations on the current and emerging computers.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Nov 6, 2025</time>
        <div class="title">
          Programming with first-class rounding contexts
        </div>
        <div class="speaker">
          Brett Saiki, University of Washington and Intel
        </div>
      </summary>
      <div class="abstract">
        <p>
          Precise specification of numerical algorithms is essential for
          design-space exploration, simulation, and verification of possible
          designs. FPCore, a Scheme-like language that makes rounding explicit and
          separate from exact mathematical operations, provides one such high-level
          abstraction. However, its rounding context construct cannot capture a key
          feature of an emerging class of algorithms: rounding that depends on
          runtime values. To express and study these designs, we propose making
          rounding contexts first-class values.
        </p>
        <p>
          We implement these ideas in FPy, a domain-specific language for exploring
          numerical algorithms. FPy allows programmers to capture both existing
          FPCore programs, and algorithms with value-dependent rounding that cannot
          be captured in FPCore. First-class rounding contexts enable systematic
          exploration, since rounding parameters may be program inputs rather than
          controlled via ad-hoc metaprogramming. We demonstrate FPy’s effectiveness
          by analyzing how design parameters affect numerical error on a family of
          designs from prior work that features value-dependent rounding.
        </p>
        <p>
          Please see the
          <a href="https://github.com/bksaiki/fpy">repo</a>
          for more info.
          Try out the pip package
          <a href="https://pypi.org/project/fpy2/">fpy2</a>!
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Oct 2, 2025</time>
        <div class="title">
          RLibm-MultiRound: correct, rounding mode-agnostic libraries
        </div>
        <div class="speaker">
          Sehyeok Park, Rutgers University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Our RLibm project generates a single implementation for an elementary
          function that produces correctly rounded results for multiple rounding
          modes and representations with up to 32-bits. The key insight is to
          build polynomials that produce the correctly rounded result for a
          representation with two additional bits when compared to the largest
          target representation and with the "non-standard" round-to-odd rounding
          mode, which makes double rounding the RLibm math library result to any
          smaller target representation innocuous. The resulting approximations
          are implemented with machine supported floating-point operations with
          the round-to-nearest rounding mode. When an application uses any other
          rounding mode, RLibm saves the application's rounding mode, changes the
          system's rounding mode to round-to-nearest, computes the correctly
          rounded result, and restores the application's rounding mode. This
          frequent change of rounding modes has a performance cost.
        </p>
        <p>
          In this talk I will cover two new methods to avoid the frequent changes
          to the rounding mode and the dependence on round-to-nearest. First, our
          new rounding-invariant outputs method emulates round-to-zero under all
          rounding modes to implement RLibm's polynomial approximations. Second,
          our rounding-invariant input bounds method factors any rounding error
          due to different rounding modes using interval bounds in the RLibm
          pipeline. Both methods make a different set of trade-offs and improve
          the performance of resulting libraries by more than 2X.
        </p>
        <p>
          Please see the
          <a href="https://arxiv.org/abs/2504.07409">paper</a>
          for more info.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Sep 4, 2025</time>
        <div class="title">
          Bean: a language for backward error analysis
        </div>
        <div class="speaker">
          Laura Zielinski, Cornell University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Backward error analysis offers a method for assessing the quality of
          numerical programs in the presence of floating-point rounding errors.
          However, techniques from the numerical analysis literature for
          quantifying backward error require substantial human effort, and there
          are currently no tools or automated methods for statically deriving
          sound backward error bounds. To address this gap, we propose Bean, a
          typed first-order programming language designed to express quantitative
          bounds on backward error. Bean's type system combines a graded coeffect
          system with strict linearity to soundly track the flow of backward
          error through programs. We prove the soundness of our system using a
          novel categorical semantics, where every Bean program denotes a triple
          of related transformations that together satisfy a backward error
          guarantee.
        </p>
        <p>
          To illustrate Bean's potential as a practical tool for automated
          backward error analysis, we implement a variety of standard algorithms
          from numerical linear algebra in Bean, establishing fine-grained
          backward error bounds via typing in a compositional style. We also
          develop a prototype implementation of Bean that infers backward error
          bounds automatically. Our evaluation shows that these inferred bounds
          match worst-case theoretical relative backward error bounds from the
          literature, underscoring Bean's utility in validating a key property of
          numerical programs: numerical stability.
        </p>
        <p>
          Please see the
          <a href="https://arxiv.org/abs/2501.14550">paper</a> and
          <a href="https://github.com/Athena-Types/Bean">code</a>
          for more info.
        </p>
      </div>
    </details>


    <details>
      <summary>
        <time>Aug 07, 2025</time>
        <div class="title">
          Numerical Computing with IEEE Floating Point Arithmetic
        </div>
        <div class="speaker">
          Michael Overton, New York University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Although the basic principles of IEEE floating point arithmetic have
          remained largely unchanged since the first edition of my book
          Numerical Computing with IEEE Floating Point Arithmetic was published
          by SIAM in 2001, the technology that supports it has changed
          enormously. Every chapter of the book has been rewritten extensively,
          and two new chapters have been added: one on computations with higher
          precision than that mandated by the standard, needed for a variety of
          scientific applications, and one on computations with lower precision
          than was ever contemplated by those who wrote the standard, driven by
          the massive computational demands of machine learning. Topics include
          the rationale for floating point representation, correctly rounded
          arithmetic and exception handling, support for the standard by
          floating point microprocessors and programming languages, and an
          introduction to the key concepts of cancellation, conditioning and
          stability. The book gives many technical details that are not readily
          available elsewhere. The second edition was published by SIAM in May
          2025.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Aug 07, 2025</time>
        <div class="title">
          Detecting and diagnosing FP exceptions in GPUs and CPUs
        </div>
        <div class="speaker">
          Mark Baranowski and Ganesh Gopalakrishnan, University of Utah
        </div>
      </summary>
      <div class="abstract">
        <p>
          Hundreds of GPU programmers are being stymied by the NaNs and INFs that
          arise during computation, often polluting loss functions (ML) and
          residuals (HPC). The debugging problem is exacerbated due to GPU kernels
          being closed-source and launched from scripts written in Python, Julia
          etc. While one may build binary analysis tools to analyze exceptions,
          separate tools are needed for different GPUs. Finally one likes to
          detect exceptions at a higher level (e.g., LLVM): the lack of publicly
          available GPU support from LLVM makes such tools more easily
          CPU-targetable.
        </p>
        <p>
          In this talk, we will briefly survey tools that can help detect and
          diagnose floating-point exceptions. The bulk of this talk will be
          devoted to covering the tools written at Utah: namely GPU-FPX (for GPU
          SIMT cores) and its 'nixnan' variant (for GPU Tensor Cores). We run a
          few demos that illustrate the ease of use of GPU-FPX on a variety of
          codes: simple data compressors, simple GPTs, and Python/Julia codes.
          While GPU-FPX currently helps ``X-Ray'' down the stack of kernel calls,
          knowing what these kernels do and which of the detected exceptions are
          relevant -- and which exception coercion rules (to normal values) are
          sound -- remains unsolved. The only clear guidance we know of --
          consistent exception handling due to Demmel -- does not seem to hold and
          is inefficient if literally followed. Given that exceptions occur with
          such high frequencies already and will multiply in their manifestations
          on different hardware and software, clear guidelines for exception
          coercion and blame assignment are needed.
        </p>
        <p>
          The talk will highlight how we mined exceptions from Tensor Cores in
          nixnan (Reference:
          <a href="https://github.com/ganeshutah/PLDI25-Array-Workshop">github.com/ganeshutah/PLDI25-Array-Workshop</a>
          ), and also summarize FloatGuard (AMD Exception Checking tool from UC Davis:
          HPDC'25) and FPChecker (LLVM Exception Checking from Livermore:
          ISSWC'22). We will devote ~15 mins to garner audience feedback to help
          us prepare for our SC'25 tutorial on this topic this November in St.
          Louis, MO.
        </p>
        <p>
          Additional input from:
          Xinyi Li (Utah),
          Dolores Miao (UC Davis),
          Harvey Dam (Utah),
          Cindy Rubio-Gonzalez (UC Davis),
          and Ignacio Laguna (LLNL).
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Jul 10, 2025
        </time>
        <div class="title">
          <a href="talks/fptalks25.html">FPTalks 2025 annual workshop</a>
        </div>
        <div class="speaker">
          12 speakers at the cutting edge of numerics research
        </div>
      </summary>
    </details>


    <details>
      <summary>
        <time>May 1, 2025</time>
        <div class="title">
          Fast branch-free extended-precision floating-point arithmetic
        </div>
        <div class="speaker">
          David K. Zhang, Standford University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Many scientific and mathematical problems demand extremely precise calculations
          exceeding the limits of the double precision (IEEE binary64) floating-point format.
          However, existing methods for extended-precision computation involve complex
          branching algorithms that perform poorly on modern data-parallel processors,
          including SIMD CPUs and GPUs. In this talk, I introduce a class of algorithms
          called floating-point accumulation networks (FPANs) that enable fast branch-free
          extended-precision arithmetic. FPAN-based algorithms outperform standard multiprecision
          libraries by orders of magnitude, achieving up to 11.7x the peak performance of QD,
          34.4x over CAMPARY, 35.6x over MPFR, and 41.4x over FLINT. I also introduce a new
          formal verification technique that leverages automatic theorem provers to rigorously
          establish the correctness of these new algorithms.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Apr 3, 2025</time>
        <div class="title">
          Fast sound error bounds for mixed-precision real evaluation
        </div>
        <div class="speaker">
          Artem Yadrov, University of Utah
        </div>
      </summary>
      <div class="abstract">
        <p>
          Evaluating real-valued expressions to high precision is a key
          building block in computational mathematics, physics, and numerics.
          A typical implementation uses a uniform precision for each
          operation, and doubles that precision until the real result can be
          bounded to some sufficiently narrow interval. However, this is
          wasteful: usually only a few operations really need to be performed
          at high precision, and the bulk of the expression could use much
          lower precision. Uniform precision can also waste iterations
          discovering the necessary precision and then still overestimate by
          up to a factor of two. We propose to instead use mixed-precision
          interval arithmetic to evaluate real-valued expressions. A key
          challenge is deriving the mixed-precision assignment both soundly
          and quickly. To do so, we introduce a sound variation of error
          Taylor series and condition numbers, specialized to interval
          arithmetic, that can be evaluated with minimal overhead thanks to
          an “exponent trick”. Our implementation, Reval, achieves an
          average speed-up of 1.47× compared to the state-of-the-art Sollya
          tool, with the speed-up increasing to 4.92× on the most difficult
          input points. An examination of the precisions used with and without
          precision tuning shows that the speed-up results come from quickly
          assigning lower precisions for the majority of operations
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Mar 6, 2025</time>
        <div class="title">
          Floating-Point Neural Network Verification
        </div>
        <div class="speaker">
          Edoardo Manino, The University of Manchester
        </div>
      </summary>
      <div class="abstract">
        <p>
          Safety-critical systems with neural network components require strong
          guarantees. While existing verification techniques have shown great
          progress towards this goal, they mostly reason on real-valued
          abstractions of neural networks. As soon as we consider their
          floating-point behaviour, the associated verification problem becomes
          harder. In this talk, we discuss a software verification approach to
          this problem. In doing so, we introduce NeuroCodeBench, a benchmark
          of neural network code for software verification. With it, we show
          the advantages and shortcomings of reasoning on neural networks at
          the floating-point level.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Feb 6, 2025
        </time>
        <div class="title">
          LP meets PL: efficient linear programming using a geometric lens
        </div>
        <div class="speaker">
          Mridul Aanjaneya, Rutgers University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Linear programs (LP) arise in many domains, such as robotics,
          databases, machine learning, computer graphics, etc. LP solvers are
          also widely used in the PL community for analyzing vulnerabilities in C
          source code, designing correctly rounded Math libraries, repairing deep
          neural networks, Presburger arithmetic for polyhedral compilation, etc.
          Linear programs with billions of constraints are beyond the reach of
          modern LP solvers. However, for many practical applications, the linear
          program is "low-dimensional", i.e., it has many constraints but only a
          few unknown variables. In this talk, we'll first discuss how random
          sampling can be used for provably solving (in terms of expected
          iterations) feasible low-dimensional linear programs with billions of
          constraints. Next, we'll discuss how this approach requires
          enhancements for infeasible linear programs, where a small number of
          conflicting constraints make the linear program have no solution. We'll
          show that a geometric perspective can be used for efficiently
          eliminating the conflicting constraints and computing a solution that
          satisfies the maximum number of constraints. We'll present applications
          of our LP solver for generating correctly rounded Math libraries for
          floating-point variants. Time permitting, we'll also discuss
          applications in interpretable machine learning.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Dec 5, 2024
        </time>
        <div class="title">
          Aster: sound mixed fixed-point quantizer for neural networks
        </div>
        <div class="speaker">
          Debasmita Lohar, Karlsruhe Institute of Technology
        </div>
      </summary>
      <div class="abstract">
        <p>
          Neural networks are increasingly becoming integral to safety-critical
          applications, such as controllers in embedded systems. While formal
          safety verification focuses on idealized real-valued networks, practical
          applications require quantization to finite precision, inevitably
          introducing roundoff errors. Manually optimizing precision, especially
          for fixed-point implementation, while ensuring safety is complex and
          time-consuming.
        </p>
        <p>
          In this talk, I will introduce Aster, the sound, fully automated,
          mixed-precision, fixed-point quantizer for deep feed-forward neural
          networks. Aster reduces the quantization problem to a mixed-integer
          linear programming (MILP) problem, thus efficiently determining minimal
          precision to guarantee predefined error bounds. Our evaluations show
          that Aster's optimized code reduces machine cycles when compiled to an
          FPGA with a commercial HLS compiler compared to (sound)
          state-of-the-art. Furthermore, Aster handles significantly more
          benchmarks faster, especially for larger networks with thousands of
          parameters.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Nov 7, 2024</time>
        <div class="title">
          Exploring FPGA designs for MX and beyond
        </div>
        <div class="speaker">
          Ebby Samson, Imperial College London
        </div>
      </summary>
      <div class="abstract">
        <p>
          A number of companies recently worked together to release the new Open
          Compute Project MX standard for low-precision computation, aimed at
          efficient neural network implementation. In our work, we describe and
          evaluate the first open-source FPGA implementation of the arithmetic
          defined in the standard. Our designs fully support all the standard's
          concrete formats for conversion into and out of MX formats and for the
          standard-defined arithmetic operations, as well as arbitrary
          fixed-point and floating-point formats. Certain elements of the
          standard are left as implementation-defined, and we present the first
          concrete FPGA-inspired choices for these elements. Our library of
          optimized hardware components is available open source, alongside our
          open-source Pytorch library for quantization into the new standard,
          integrated with the Brevitas library so that the community can develop
          novel neural network designs quantized with MX formats in mind. Our
          testing shows that MX is very effective for formats such as INT5 or FP6
          which are not natively supported on GPUs. This gives FPGAs an advantage
          as they have the flexibility to implement a custom datapath and take
          advantage of the smaller area footprints offered by these formats.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Oct 3, 2024</time>
        <div class="title">
          Geometric predicates for unconditional elastodynamics simulation
        </div>
        <div class="speaker">
          Daniele Panozzo, Courant Institute of Mathematical Sciences in New York University
        </div>
      </summary>
      <div class="abstract">
        <p>
          The numerical solution of partial differential equations (PDE) is
          ubiquitously used for physical simulation in scientific computing and
          engineering. Ideally, a PDE solver should be opaque: the user provides
          as input the domain boundary, boundary conditions, and the governing
          equations, and the code returns an evaluator that can compute the
          value of the solution at any point of the input domain. This is
          surprisingly far from being the case for all existing open-source or
          commercial software, despite the research efforts in this direction
          and the large academic and industrial interest. To a large extent,
          this is due to lack of robustness in geometric algorithms used to
          create the discretization, detect collisions, and evaluate element
          validity.
        </p>
        <p>
          I will present the incremental potential contact simulation paradigm,
          which provides strong robustness guarantees in simulation codes,
          ensuring, for the first time, validity of the trajectories accounting
          for floating point rounding errors over an entire elastodynamic
          simulation with contact. A core part of this approach is the use of a
          conservative line-search to check for collisions between geometric
          primitives and for ensuring validity of the deforming elements over
          linear trajectories.
        </p>
        <p>
          I will discuss both problems in depth, showing that SOTA approaches
          favor numerical efficiency but are unfortunately not robust to
          floating point rounding, leading to major failures in simulation. I
          will then present an alternative approach based on judiciously using
          rational and interval types to ensure provable correctness, while
          keeping a running time comparable with non-conservative methods.
          To conclude, I will discuss a set of applications enabled by this
          approach in microscopy and biomechanics, including traction force
          estimation on a live zebrafish and efficient modeling and simulation
          of fibrous materials.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>Sep 5, 2024</time>
        <div class="title">
          Designing Type Systems for Rounding Error Analysis
        </div>
        <div class="speaker">
          Ariel Kellison, Cornell University
        </div>
      </summary>
      <div class="abstract">
        <p>
          A central challenge in scientific computing is managing the tradeoff between accuracy
          and performance. Scientific applications often require high precision to produce
          meaningful results, but achieving this precision can reduce computational speed
          and efficiency. For example, using higher precision arithmetic can minimize rounding
          errors and improve the reliability of results, but it typically demands more processing
          power and memory, which negatively affects performance. As a result, scientific software
          developers must carefully balance the need for accuracy with the need for acceptable
          performance.
        </p>
        <p>
          In recent years, programming languages like Rust have demonstrated how carefully
          designed type systems can help developers write high-performance code while ensuring
          critical properties, such as memory safety. In this talk, we will present our work on
          designing type systems that provide guarantees about the accuracy of floating-point
          computations. By introducing types that can quantitatively represent the error bounds
          of floating-point computations, we can create statically typed programming languages
          that alert developers to potentially significant inaccuracies early in the development
          process.
        </p>
      </div>
    </details>


    <details>
      <summary>
        <time>
          Jul 11, 2024
        </time>
        <div class="title">
          <a href="talks/fptalks24.html">FPTalks 2024 annual workshop</a>
        </div>
        <div class="speaker">
          12 speakers at the cutting edge of numerics research
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Jun 6, 2024
        </time>
        <div class="title">
          On the precision loss in approximate homomorphic encryption
        </div>
        <div class="speaker">
          Rachel Player, Royal Holloway University of London
        </div>
      </summary>
      <div class="abstract">
        <p>
          Since its introduction at Asiacrypt 2017, the CKKS approximate homomorphic encryption
          scheme has become one of the most widely used and implemented homomorphic encryption
          schemes. Due to the approximate nature of the scheme, application developers using
          CKKS must ensure that the evaluation output is within a tolerable error of the corresponding
          plaintext computation. Choosing appropriate parameters requires a good understanding of
          how the noise will grow through the computation. A strong understanding of the noise
          growth is also necessary to limit the performance impact of mitigations to the attacks
          on CKKS presented by Li and Micciancio (Eurocrypt 2021).
        </p>
        <p>
          In this work we present a
          comprehensive noise analysis of CKKS, that considers noise coming both from the encoding and
          homomorphic operations. Our main contribution is the first average-case analysis for CKKS
          noise, and we also introduce refinements to prior worst-case noise analyses. We develop
          noise heuristics both for the original CKKS scheme and the RNS variant presented
          at SAC 2018. We then evaluate these heuristics by comparing the predicted noise growth
          with experiments in the HEAAN and FullRNS-HEAAN libraries, and by comparing with a
          worst-case noise analysis as done in prior work. Our findings show mixed results: while
          our new analyses lead to heuristic estimates that more closely model the observed noise
          growth than prior approaches, the new heuristics sometimes slightly underestimate the
          observed noise growth. This evidences the need for implementation-specific noise analyses
          for CKKS, which recent work has shown to be effective for implementations of similar schemes.
          This is joint work with Anamaria Costache, Benjamin R. Curtis, Erin Hales, Sean Murphy,
          and Tabitha Ogilvie.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          May 2, 2024
        </time>
        <div class="title">
          ReLU hull approximation
        </div>
        <div class="speaker">
          Zhongkui Ma, The University of Queensland
        </div>
      </summary>
      <div class="abstract">
        <p>
          Neural networks have offered distinct advantages over traditional
          techniques. However, the opaque neural networks render their
          performance vulnerable, in the presence of adversarial samples. This
          underscores the imperative of formal verification of neural networks,
          especially in the safety-critical domain. Deep neural networks are
          composed of multiple hidden layers with neurons, where mathematical
          operations stack linear and nonlinear (activation functions)
          operations. Using linear inequalities plays a crucial role in
          constraining and deducing the ranges of results from nonlinear
          operations. Considering correlations among input variables of multiple
          neurons leads to constraints known as multi-neuron constraints, posing
          a non-trivial high-dimensional challenge in computing the convex hull
          of functions. This work is dedicated to designing methods for computing
          multi-neuron constraints for the ReLU function to serve the robustness
          verification of neural networks. We have introduced a novel approach
          WRALU (Wrapping ReLU) for computing the convex hull of a ReLU function
          (<a
          href='https://popl24.sigplan.org/details/POPL-2024-popl-research-papers/77/ReLU-Hull-Approximation'>published
          in POPL’24</a>).
          This method significantly reduces computation
          time and complexity, constructing fewer constraints to achieve more
          precise approximations while handling higher dimensions.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Apr 4, 2024
        </time>
        <div class="title">
          Rigorous error analysis for logarithmic number systems
        </div>
        <div class="speaker">
          Thanh Son Nguyen, University of Utah
        </div>
      </summary>
      <div class="abstract">
        <p>
          Logarithmic Number Systems (LNS) hold considerable promise in
          helping reduce the number of bits needed to represent a high
          dynamic range of real-numbers with finite precision, and also
          efficiently support multiplication and division. However,
          under LNS, addition and subtraction turn into non-linear
          functions that must be approximated—typically using
          precomputed table-based functions. Additionally, multiple
          layers of error correction are typically needed to improve
          result accuracy. Unfortunately, previous efforts have not
          characterized the resulting error bound. We provide the first
          rigorous analysis of LNS, covering detailed techniques such as
          co-transformation that are crucial to implementing subtraction
          with reasonable accuracy. We provide theorems capturing the
          error due to table interpolations, the finite precision of
          pre-computed values in the tables, and the error introduced by
          fix-point multiplications involved in LNS implementations. We
          empirically validate our analysis using a Python
          implementation, showing that our analytical bounds are tight,
          and that our testing campaign generates inputs diverse-enough
          to almost match (but not exceed) the analytical bounds. We
          close with discussions on how to adapt our analysis to LNS
          systems with different bases and also discuss many pragmatic
          ramifications of our work in the broader arena of scientific
          computing and machine learning.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Mar 7, 2024
        </time>
        <div class="title">
          Low-bit quantization for efficient and accurate LLM serving
        </div>
        <div class="speaker">
          Chien-Yu Lin, University of Washington
        </div>
      </summary>
      <div class="abstract">
        <p>
          The growing demand for Large Language Models (LLMs) in
          applications such as content generation, intelligent chatbots,
          and sentiment analysis poses considerable challenges for LLM
          service providers. To efficiently use GPU resources and boost
          throughput, batching multiple requests has emerged as a
          popular paradigm; to further speed up batching, LLM
          quantization techniques reduce memory consumption and increase
          computing capacity. However, prevalent quantization schemes
          (e.g., 8-bit weight-activation quantization) cannot fully
          leverage the capabilities of modern GPUs, such as 4-bit
          integer operators, resulting in sub-optimal performance.
        </p>
        <p>
          To maximize LLMs' serving throughput, we introduce Atom, a
          low-bit quantization method that achieves high throughput
          improvements with negligible accuracy loss. Atom significantly
          boosts serving throughput by using low-bit operators and
          considerably reduces memory consumption via low-bit
          quantization. It attains high accuracy by applying a novel
          mixed-precision and fine-grained quantization process. We
          evaluate Atom on 4-bit weight-activation quantization setups
          in the serving context. Atom improves end-to-end throughput by
          up to 7.73× compared to the FP16 and by 2.53× compared to INT8
          quantization, while maintaining the same latency target.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Feb 1, 2024
        </time>
        <div class="title">
          Towards optimized multiplierless arithmetic circuits
        </div>
        <div class="speaker">
          Rémi Garcia, University of Rennes
        </div>
      </summary>
      <div class="abstract">
        <p>
          Multiplication is a basic operator used in many applications. When
          implemented in embedded systems, e.g. FPGAs, these algorithms require
          highly optimized hardware. Improving the multiplication implementation
          is an important part of the final hardware cost reduction. This talk
          will expose the recent advances on the automatic design of
          multiplication of a variable with multiple <emph>a priori</emph> known
          constants, this problem is called the Multiple Constant Multiplication
          (MCM) problem. Using Integer Linear Programming (ILP) permits to
          significantly reduce the hardware cost when MCM is implemented using
          additions and bit-shifts. The ILP approach has been efficiently used
          for various hardware design problems but other approaches exists, such
          as SAT/SMT, Constraint Programming, etc. This presentation will
          introduce a few concepts relating to this ecosystem.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Dec 7, 2023
        </time>
        <div class="title">
          Application-specific arithmetic
        </div>
        <div class="speaker">
          Florent de Dinechin, INSA-Lyon/Inria; and Martin Kumm, Fulda University
        </div>
      </summary>
      <div class="abstract">
        <p>
          A software designer has to carefully select the arithmetic that best
          suits each step of her application, but the choice is constrained: only
          a handful of operators are supported in hardware, in only in a handful
          of precisions.
          Conversely, when designing for hardware or FPGAs, these constraints
          become degrees of freedom. Any precision can be designed, any number
          encoding, but also any operation and function, provided you are clever
          enough to implement it efficiently as a circuit.
        </p>
        <p>
          This talk will expose some of the opportunities offered by this freedom,
          and some of the methodologies and tools that allow a designer to build
          and compose application-specific operators.
          It may include some inadvertent advertising for our upcoming 800-page
          book dedicated to these topics.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Nov 2, 2023
        </time>
        <div class="title">
          Overview of numerics on the Java Platform
        </div>
        <div class="speaker">
          Joseph D. Darcy, Oracle
        </div>
      </summary>
      <div class="abstract">
        <p>
          Come hear an overview of numeric support of the Java platform, an on-going
          story ranging over more than a quarter century and spanning standards,
          specifications, virtual machines, libraries, and compilers, including resolving
          details as small as 2^-1074. The talk does not assume audience members will
          already be familiar with the details of the Java platform; attendees are welcome
          to ask questions about such details.
        </p>
        <p>
          Joe is a long-time member on the JDK engineering team, first at Sun and later
          Oracle. As “Java Floating-Point Czar” he has looked after Java numerics including
          contributing to the design of strictfp, adding numerous floating-point math library
          methods, and adding hexadecimal floating-point literals to the Java language and
          library. Joe was a participant in and interim editor of the 2008 revision to the
          IEEE 754 floating-point standard. Outside of numerics, Joe has done other
          foundational work on the Java platform including core libraries development, Java
          language changes including Project Coin, infrastructure improvements, and reviewing
          platform interface updates, together with a smattering of project management and
          release management along the way.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Oct 5, 2023
        </time>
        <div class="title">
          Custom elementary functions for hardware accelerators
        </div>
        <div class="speaker">
          Benjamin Carleton and Adrian Sampson, Cornell University
        </div>
      </summary>
      <div class="abstract">
        <p>
          This talk is a work-in-progress report about our efforts to
          generate customized hardware implementations of fixed-point
          function approximations for application-specific accelerators.
          We built a new compiler from FPCore
          to <a href="https://calyxir.org/">Calyx</a>, our lab’s IR and
          compiler for translating high-level descriptions to hardware
          designs. The interesting part is the need to generate
          efficient polynomial approximations for elementary
          functions—and the freedom to customize these implementations
          for the specific accelerator’s context. We have preliminary
          (but encouraging) comparisons to a commercial high-level
          synthesis (HLS) compiler. We will seek feedback on the many
          possibilities for next steps.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Sep 7, 2023
        </time>
        <div class="title">
          A PVS formalization of Taylor error expressions
        </div>
        <div class="speaker">
          Jonas Katona, Yale University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Due to the limits of finite precision arithmetic, as one works
          with floating-point representations of real numbers in
          computer programs, round-off errors tend to accumulate under
          mathematical operations and may become unacceptably large.
          Symbolic Taylor Error Expressions is a technique used in the
          tool FPTaylor to bound the round-off errors accumulated under
          differentiable mathematical operations which provides a good
          trade-off between efficiency and accuracy. In this
          presentation, I will present a formally verified
          implementation of Symbolic Taylor Error Expressions in a
          specification language, automated theorem prover, and
          typechecker called Prototype Verification System (PVS). I will
          also go over how this formal specification in PVS will enable
          the integration of Symbolic Taylor Expressions in PRECiSA, a
          prototype static analysis tool that can compute verified
          round-off error bounds for floating-point programs.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Aug 3, 2023
        </time>
        <div class="title">
          Automated reasoning over the reals
        </div>
        <div class="speaker">
          Soonho Kong, Amazon Web Services
        </div>
      </summary>
      <div class="abstract">
        <p>
          In cyber-physical systems such as autonomous driving systems
          and robotics components, ordinary differential equations and
          nonlinear functions such as the exponential function and the
          trigonometric functions appear almost universally. To adopt
          automated reasoning techniques in these domains, it is
          necessary to develop a tool that can handle logical encodings
          with those functions.
        </p>
        <p>
          In this talk, I will present the design and implementation of
          a delta-decision procedure, dReal. First, we will discuss the
          theory of delta-decidability briefly. Then I will explain 1)
          how the tool handles nonlinear functions and ordinary
          differential equations, 2) how it can solve optimization and
          synthesis problems (∃∀-formulas), and 3) how the tool utilizes
          multiple cores to speed up the solving process in parallel. I
          will show that the tool helps tackle instances from real-world
          applications including bounded model-checking for
          cyber-physical systems, nonlinear global optimization, and
          Lyapunov-stability analysis in robotics.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Jul 6, 2023
        </time>
        <div class="title">
          <a href="talks/fptalks23.html">FPTalks 2023 annual workshop</a>
        </div>
        <div class="speaker">
          12 speakers at the cutting edge of numerics research
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Jun 1, 2023
        </time>
        <div class="title">
          Floating-point education roundtable
        </div>
        <div class="speaker">
          Mike Lam, James Madison University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Members of the FPBench community are well-aware of the
          pitfalls of floating-point representation, but awareness among
          the wider population of software authors is
          often <a href="https://doi.org/10.1109/IPDPS.2018.00068">painfully
          inadequate</a>. There is a surprising dearth of quality
          educational materials and techniques for teaching these issues
          to students (both undergraduate and graduate). In this
          roundtable discussion, we’ll look at some of the current
          approaches and brainstorm possible approaches to improve the
          state of the art.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          May 4, 2023
        </time>
        <div class="title">
          Floating-point accuracy anecdotes from real-world products
        </div>
        <div class="speaker">
          Shoaib Kamil, Adobe Research
        </div>
      </summary>
      <div class="abstract">
        <p>
          Floating point issues must often be surmounted when building
          software products that rely on precise computations while
          still demanding performance. In this talk, I describe two
          scenarios where existing tools fall short. First, I will
          describe our efforts to utilize interval computation to better
          bound quantities representing real numbers, and the rabbit
          hole we encountered when trying to find an existing
          off-the-shelf cross-platform interval computation library.
          From this exploration, we built a cross-platform interval
          benchmark suite using hand-crafted expressions from real-world
          code and from FPBench, and evaluated four existing interval
          libraries for correctness, interval width, and speed. Second,
          I will describe our experiences using GPU floating point math
          to emulate integer computations, and the pitfalls even when
          computing with exactly-representable floating point numbers.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Apr 6, 2023
        </time>
        <div class="title">
          Formal and semi-formal verification of C numerics
        </div>
        <div class="speaker">
          Samuel D. Pollard and Ariel Kellison,
          Sandia National Laboratories
        </div>
      </summary>
      <div class="abstract">
        <p>
          For better or worse, the world runs on C, and to a lesser
          extent, C with IEEE 754 floating point. At Sandia Labs, we
          often are tasked with the formal verification of
          high-consequence programs in C. Two methods we use at Sandia
          to accomplish this verification are: fully constructive proofs
          of correctness (in the Coq theorem prover), and deductive
          verification (using Frama-C). In both cases, the addition of
          numerical computations adds complexity to the programs, via
          numerical error (e.g., discretization or roundoff error), or
          run-time errors (such as floating-point exceptions). We
          discuss our efforts at Sandia Labs to both make numerical
          models of C programs using Frama-C and its ANSI C
          specification language (ACSL), as well as functional models in
          Coq, and how to check whether the relevant C code implements
          these specifications.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Mar 2, 2023
        </time>
        <div class="title">
          Automatically generating numerical PDE solvers with Finch
        </div>
        <div class="speaker">
          Eric Heisler,
          University of Utah
        </div>
      </summary>
      <div class="abstract">
        <p>
          Finch is a domain-specific language for numerically solving differential equations. It employs a variety of
          numerical techniques, generates code for a number of different target frameworks and languages, and allows
          almost arbitrary equation input. In addition, it accepts unrestricted modification of the generated code to
          provide maximal flexibility and hand-optimization. One downside to this (perhaps excessive) flexibility is
          that numerical instability and the occasional typo can easily turn a solution into a pile of NaNs.
          Eric will introduce Finch, and demonstrate some of the features relevant to the numerics. We will look at
          some examples where things turn out just the way we hoped, and some where they don't.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Dec 8, 2022
        </time>
        <div class="title">
          Low-precision FP8 formats: background and industry status
        </div>
        <div class="speaker">
          Marius Cornea, Intel
        </div>
      </summary>
      <div class="abstract">
        <p>
          Marius Cornea, a senior principal engineer at Intel working on math libraries and floating-point (and one of the
          coauthors of the IEEE Standard 754-2008), will be presenting about new industry standards for low-precision
          formats. The presentation will examine the reasons for which FP8 formats have emerged, definition details, and
          variations proposed by industry and academia. We will enumerate a few known (mostly planned) implementations of
          FP8. We will also look at current standardization work for FP8 formats.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Nov 18, 2022
        </time>
        <div class="title">
          Correctness 2022 annual workshop
        </div>
        <div class="speaker">
          Workshop on HPC software correctness, co-located with SC
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Nov 3, 2022
        </time>
        <div class="title">
          Function-Based volumetric design and fabrication
        </div>
        <div class="speaker">
          Chris Uchytil,
          Mechanical Engineering,
          University of Washington
        </div>
      </summary>
      <div class="abstract">
        <p>
          We present a novel computer-aided design (CAD) modeling system designed
          to support a modeling range that matches the fabrication range of
          modern additive manufacturing (AM) systems that are capable of
          producing large-scale, high-resolution parts. To be useful to designers
          and fabricators, a modeling system must perform essential functions
          (such as execution of modeling operations, visualization, and slicing)
          at interactive rates, and achieving the necessary performance depends
          on efficient use of both memory and computing capacity. Our approach to
          achieving necessary performance levels is to implement an implicit
          function-based representation (f-rep) modeling system, not using a
          computer algebra system, but instead using just-in-time (JIT)
          compilation of user-defined functions. Efficient memory usage is
          achieved via a sparse volume data structure that builds on previous
          work by Hoetzlein [Hoetzlein 2016]. Computational efficiency is
          achieved through a combination of interval arithmetic (IA) and
          massively parallel evaluation on the GPU. We follow [Keeter 2020] and
          employ IA as the basis for local pruning of the function evaluation
          tree to minimize the required number of function evaluations, and we
          take advantage of GPU-parallelism to significantly enhance
          computational throughput.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Oct 6, 2022
        </time>
        <div class="title">
          Finding inputs that trigger GPU exceptions
        </div>
        <div class="speaker">
          Ignacio Laguna,
          Center for Applied Scientific Computing,
          LLNL
        </div>
      </summary>
      <div class="abstract">
        <p>
          Testing code for floating-point exceptions is crucial as exceptions can
          quickly propagate and produce unreliable numerical answers. The
          state-of-the-art to test for floating-point exceptions in GPUs is quite
          limited and solutions require the application's source code, which
          precludes their use in accelerated libraries where the source is not
          publicly available. We present an approach to find inputs that trigger
          floating-point exceptions in black-box GPU functions, i.e., functions
          where the source code and information about input bounds are
          unavailable. Our approach is the first to use Bayesian optimization
          (BO) to identify such inputs and uses novel strategies to overcome the
          challenges that arise in applying BO to this problem. We implement our
          approach in the Xscope framework and demonstrate it on 58 functions
          from the CUDA Math Library and functions from ten HPC programs. Xscope
          is able to identify inputs that trigger exceptions in about 72% of the
          tested functions.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Sep 2, 2022
        </time>
        <div class="title">
          Birds-of-a-feather meeting
        </div>
        <div class="speaker">
          Discussing challenges in connecting analysis tools
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Aug 4, 2022
        </time>
        <div class="title">
          Database workbench:
          mixed-initiative design space search
        </div>
        <div class="speaker">
          Edward Misback,
          University of Washington
        </div>
      </summary>
      <div class="abstract">
        <p>
          The Herbie web demo is intended as an accessible tool for making a
          rough guess at the best way to improve the accuracy of a floating-point
          expression. It aims to support students learning about floating point
          error, casual users who just want a high-accuracy replacement
          expression, and expert users who are comfortable interpreting its
          results. However, it is designed as a single-step tool and leaves
          little room for users to iterate or adapt it to numerical analysis
          tasks, even though its underlying functions have broader applications.
          We describe how an alternative “database workbench” interface will
          allow users to leverage Herbie’s power while exploring the design space
          of replacement expressions more flexibly, including support for the
          development of third-party expression analysis plugins. We would like
          to discuss with the FPBench community whether the workflow we describe
          is compatible with other numerical analysis tools and needs, either as
          plugins or as separate workbenches with a similar interface.
        </p>
        <p>
          <a
          href='https://docs.google.com/presentation/d/1RQ3tDkAzkoFcCZiA2iDpSuHVEJ-aYESwzwvV8M4Ec_s/edit#slide=id.p'>Slides</a>
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Jul 6, 2022
        </time>
        <div class="title">
          <a href='talks/fptalks22.html'>FPTalks 2022 annual workshop</a>
        </div>
        <div class="speaker">
          11 speakers at the cutting edge of numerics research
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          May 5, 2022
        </time>
        <div class="title">
          Automatic datapath optimization using e-graphs
        </div>
        <div class="speaker">
          Samuel Coward,
          Intel &amp Imperial College London
        </div>
      </summary>
      <div class="abstract">
        <p>
          RTL development is hampered by low design space exploration and long
          debug times. High Level Synthesis and ML techniques are not tackling
          the nature of optimization RTL teams are doing in complex Graphics type
          IP. This talk provides initial results on: automatically transforming
          RTL , exploring the design space, how Intel GFx knowhow is exploited,
          associated formal verification and how the foundation of the system is
          using the latest e-graph technology.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Apr 7, 2022
        </time>
        <div class="title">
          Choosing function implementations for speed and accuracy
        </div>
        <div class="speaker">
          Ian Briggs,
          Computer Science,
          University of Utah
        </div>
      </summary>
      <div class="abstract">
        <p>
          Standard implementations of functions like sin and exp optimize for
          accuracy, not speed, because they are intended for general-purpose use.
          But just like many applications tolerate inaccuracy from cancellation,
          rounding error, and singularities, many applications could also
          tolerate less-accurate function implementations. This raises an
          intriguing possibility: speeding up numerical code by using different
          function implementations.
        </p>
        <p>
          Enter OpTuner, an automated tool for selecting the best implementation
          for each mathematical function call site. OpTuner uses error Taylor
          series and integer linear programming to compute optimal assignments of
          297 function implementations to call sites and presents the user with a
          speed-accuracy Pareto curve. In a case study on the POV-Ray ray tracer,
          OpTuner speeds up a critical computation by 2.48×, leading to a whole
          program speedup of 1.09× with no change in the program output; human
          efforts result in slower code and lower-quality output. On a broader
          study of 36 standard benchmarks, OpTuner demonstrates speedups of 2.05×
          for negligible decreases in accuracy and up to 5.37× for error-tolerant
          applications.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Mar 3, 2022
        </time>
        <div class="title">
          CORE-MATH correctly rounded mathematical functions
        </div>
        <div class="speaker">
          Paul Zimmerman, INRIA
        </div>
      </summary>
      <div class="abstract">
        <p>
          The mission of the CORE-MATH project is to provide off-the-shelf
          open-source mathematical functions with correct rounding that can be
          integrated into current mathematical libraries (GNU libc, Intel Math
          Library, AMD Libm, Newlib, OpenLibm, Musl, Apple Libm, llvm-libc, CUDA
          libm, ROCm). This presentation covers first results in
          single-precision (binary32).
        </p>
        <p>
          <a href='https://members.loria.fr/PZimmermann/talks/'>Slides</a>
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Feb 3, 2022
        </time>
        <div class="title">
          Formal verification of numerical Hamiltonian systems
        </div>
        <div class="speaker">
          Ariel Kellison, Cornell University
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Jan 6, 2022
        </time>
        <div class="title">
          Birds-of-a-feather meeting
        </div>
        <div class="speaker">
          Discussing training and teaching numerics in academia and industry

        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Dec 2, 2021
        </time>
        <div class="title">
          Automated discovery of invertibility conditions
        </div>
        <div class="speaker">
          Andrew Reynolds, University of Iowa
        </div>
      </summary>
      <div class="abstract">
        <p>
          Satisfiability Modulo Theories (SMT) solvers have gained widespread
          popularity as reasoning engines in numerous formal methods
          applications, including in syntax-guided synthesis (SyGuS)
          applications. In this talk, we focus on an emerging class of
          applications for syntax-guided synthesis, namely, the use of a SyGuS
          solver to (partially) automate further development and improvements to
          the SMT solver itself. We describe several recent features in the SMT
          solver cvc5 that follow this paradigm. These include syntax-guided
          enumeration of rewrite rules, selection of test inputs, and discovery
          of solved forms for quantified constraints. For the latter, we describe
          how syntax-guided synthesis recently played a pivotal role in the
          development of a new algorithm for quantified bit-vector constraints
          based on invertibility conditions, and how this technique can be
          extended for similar algorithms that target quantified constraints in
          other theories, including the theory of floating points.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Nov 19, 2021
        </time>
        <div class="title">
          <a href="https://sc22.supercomputing.org/session/?sess=sess448">Correctness 2021 annual workshop</a>
        </div>
        <div class="speaker">
          Workshop on software correctness for HPC applications, co-located with SC
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Nov 4, 2021
        </time>
        <div class="title">
          Birds-of-a-feather
        </div>
        <div class="speaker">
          The community discussed the challenge of porting
          numerical applications
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Oct 7, 2021
        </time>
        <div class="title">
          Lazy exact real arithmetic using floating-point operations
        </div>
        <div class="speaker">
          Ryan McCleeary,
          Numerica
        </div>
      </summary>
      <div class="abstract">
        <p>
          Exact real arithmetic systems can specify any amount of precision on
          the output of the computations. They are used in a wide variety of
          applications when a high degree of precision is necessary. Some of
          these applications include: differential equation solvers, linear
          equation solvers, large scale mathematical models, and SMT solvers.
          This talk describes a new exact real arithmetic system which uses lazy
          list of floating point numbers to represent the real numbers. It
          details algorithms for basic arithmetic computations on these
          structures and proves their correctness. This proposed system has the
          advantage of algorithms which can be supported by modern floating point
          hardware, while still being a lazy exact real arithmetic system
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Aug 5, 2021
        </time>
        <div class="title">
          State of FPBench 2021
        </div>
        <div class="speaker">
          The community discussed FPBench benchmarks, education, and outreach
        </div>
      </summary>
      <div class="abstract">
        <p>
          In this meeting, the community discussed what the community
          might most productively focus on in the coming year. The
          discussion was lively and landed on three major thrusts:
          (1) categorizing benchmarks by domain and improving licensing,
          (2) developing "FP course modules" that instructors
          can easily incorporate into their courses, and
          (3) increasing industrial outreach to encourage FPCore adoption
          as a formal, vendor-neutral specification language.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Jul 14, 2021
        </time>
        <div class="title">
          <a href='talks/fptalks21.html'>FPTalks 2021 annual workshop</a>
        </div>
        <div class="speaker">
          19 speakers at the cutting edge of numerics research
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Jul 1, 2021
        </time>
        <div class="title">
          Scaling error analysis to billions of FLOPs
        </div>
        <div class="speaker">
          Sam Pollard, Sandia National Laboratories
        </div>
      </summary>
      <div class="abstract">
        <p>
          Sam Pollard shared his work on floating-point error analysis of kernels
          with many FLOPs (10^9 or more), by combining FPTaylor and analytical
          error bounds on inner products.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          May 6, 2021
        </time>
        <div class="title">
          RLIBM-32: fast and correct 32-bit math libraries
        </div>
        <div class="speaker">
          Jay Lim,
          Rutgers University
        </div>
      </summary>
      <div class="abstract">
        <p>
          RLIBM-32 provides a set of techniques to develop correctly rounded math
          libraries for 32-bit float and posit types. It enhances our RLibm
          approach that frames the problem of generating correctly rounded
          libraries as a linear programming problem in the context of 16-bit
          types to scale to 32-bit types. Specifically, this talk with detail new
          algorithms to (1) generate polynomials that produce correctly rounded
          outputs for all inputs using counterexample guided polynomial
          generation, (2) generate efficient piecewise polynomials with
          bit-pattern based domain splitting, and (3) deduce the amount of
          freedom available to produce correct results when range reduction
          involves multiple elementary functions. The resultant math library for
          the 32-bit float type is faster than state-of-the-art math libraries
          while producing the correct output for all inputs. We have also
          developed a set of correctly rounded elementary functions for 32-bit
          posits.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Apr 1, 2021
        </time>
        <div class="title">
          SHAMAN: evaluating numerical error for real C++ code
        </div>
        <div class="speaker">
          Nestor Demeure, University Paris Saclay
        </div>
      </summary>
      <div class="abstract">
        <p>
          <a href="https://github.com/nestordemeure/shaman">Shaman</a> is a C++11
          library that use operator overloading and a novel method to evaluate
          the numerical accuracy of an application. It has been designed to
          target high-performance simulations and, thus, we insured that it is
          not only accurate but also: is fast enough to be tested on very large
          simulations, is compatible with all of C++ mathematical functions, and
          is threadsafe and compatible with both OpenMP and MPI. In this talk,
          Nestor gave a live demo of Shaman's capabilities and discuss its novel
          design features.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Mar 4, 2021
        </time>
        <div class="title">
          POP: fast and efficient bit-level precision tuning
        </div>
        <div class="speaker">
          Dorra Ben Khalifa,
          Perpignan University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Dorra gave a great demo of <a href="">POP</a>, a tool from her thesis
          work on precision tuning. Summarizing from her abstract: POP
          establishes novel techniques for precision tuning. The main idea of our
          approach is based on semantic modeling of the propagation of the
          numerical errors throughout the program source. This yields a system of
          constraints whose minimal solution gives the best tuning of the
          program. Based on a static analysis approach, we formulate the problem
          of precision tuning with two different methods. The first method
          combines a forward and a backward error analysis which are two popular
          paradigms of error analysis. Next, our analysis is expressed as a set
          of linear constraints, made of propositional logic formulas and
          relations between integer elements only, checked by a SMT solver. The
          second method consists of generating an Integer Linear Problem (ILP)
          from the program. This is done by reasoning on the most significant bit
          and the number of significant bits of the values which are integer
          quantities. The integer solution to this problem, computed in
          polynomial time by a classical linear programming solver, gives the
          optimal data types at bit-level. A finer set of semantic equations is
          also proposed which does not reduce directly to an ILP problem. We use
          the policy iteration technique to find a solution. The POP tool
          implements both methods, and in this session Dorra demonstrateed it
          across several benchmarks coming from various application domains such
          as embedded systems, Internet of Things (IoT), physics, etc.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Feb 4, 2021
        </time>
        <div class="title">
          Keeping science on keel when software moves
        </div>
        <div class="speaker">
          Allison Baker,
          National Center for Atmospheric Research
        </div>
      </summary>
      <div class="abstract">
        <p>
          Allison lead a great discussion of her recent CACM article
          <a href="https://cacm.acm.org/magazines/2021/2/250083-keeping-science-on-keel-when-software-moves/fulltext">
          Keeping Science on Keel When Software Moves</a>.
        </p>
        <p>
          High performance computing (HPC) is central to solving large problems
          in science and engineering through the deployment of massive amounts of
          computational power. The development of important pieces of HPC
          software spans years or even decades, involving dozens of computer and
          domain scientists. During this period, the core functionality of the
          software is made more efficient, new features are added, and the
          software is ported across multiple platforms. Porting of software in
          general involves the change of compilers, optimization levels,
          arithmetic libraries, and many other aspects that determine the machine
          instructions that actually get executed. Unfortunately, such changes do
          affect the computed results to a significant (and often worrisome)
          extent. In a majority of cases, there are not easily definable a priori
          answers one can check against. A programmer ends up comparing the new
          answer against a trusted baseline previously established or checks for
          indirect confirmations such as whether physical properties such as
          energy are conserved. However, such non-systematic efforts might miss
          underlying issues, and the code may keep misbehaving until these are
          fixed.
        </p>
        <p>
          In this session, Allison presented real-world evidence to show that
          ignoring numerical result changes can lead to misleading scientific
          conclusions. She presented techniques and tools that can help
          computational scientists understand and analyze compiler effects on
          their scientific code. These techniques are applicable across a wide
          range of examples to narrow down the root-causes to single files,
          functions within files, and even computational expressions that affect
          specific variables. The developer may then rewrite the code selectively
          and/or suppress the application of certain optimizations to regain more
          predictable behavior.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Jan 7, 2021
        </time>
        <div class="title">
          Community Demos
        </div>
        <div class="speaker">
          Tanmay Tirpankar, University of Utah; and Mike Lam, James Madison University
        </div>
      </summary>
      <div class="abstract">
        <p>
          Tanmay Tirpankar
          https://github.com/arnabd88/Satire
        </p>
        <p>
          Mike Lam
          https://github.com/crafthpc/floatsmith
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Dec 3, 2020
        </time>
        <div class="title">
          Rival: an interval arithmetic for robust error estimation
        </div>
        <div class="speaker">
          Oliver Flatt,
          University of Utah
        </div>
      </summary>
      <div class="abstract">
        <p>
          Interval arithmetic is a simple way to compute a mathematical
          expression to an arbitrary accuracy, widely used for verifying
          floating-point computations. Yet this simplicity belies challenges.
          Some inputs violate preconditions or cause domain errors. Others cause
          the algorithm to enter an infinite loop and fail to compute a ground
          truth. Plus, finding valid inputs is itself a challenge when invalid
          and unsamplable points make up the vast majority of the input space.
          These issues can make interval arithmetic brittle and temperamental.
        </p>
        <p>
          Rival introduces three extensions to interval arithmetic to
          address these challenges. Error intervals express rich notions of input
          validity and indicate whether all or some points in an interval violate
          implicit or explicit preconditions. Movability flags detect futile
          recomputations and prevent timeouts by indicating whether a
          higher-precision recomputation will yield a more accurate result. And
          input search restricts sampling to valid, samplable points, so they are
          easier to find. We compare these extensions to the state-of-the-art
          technical computing software Mathematica, and demonstrate that our
          extensions are able to resolve 60.3% more challenging inputs, return
          10.2× fewer completely indeterminate results, and avoid 64 cases of
          fatal error.
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Nov 2, 2020
        </time>
        <div class="title">
          FPY: the future of FPCore
        </div>
        <div class="speaker">
          Bill Zorn,
          University of Washington
        </div>
      </summary>
      <div class="abstract">
        <p>
          FPY is a new language for specifying benchmarks that is meant to be
          easier to write for larger programs while retaining compatibility with
          existing FPCore-based tools. FPY repackages the precise semantics of
          FPCore under more familiar and convenient Python-like syntax. This
          talk will discuss the prototype design and highlight new conveniences
          FPY affords over FPCore.
        </p>
        <p>
          <a
          href='https://docs.google.com/presentation/d/15KlKe-KU-L8qZa-fJsRq06ro1JOboxwmWAPNH7IsxhY/edit#slide=id.ga74a5d7690_0_15'>Slides</a>
        </p>
      </div>
    </details>

    <details>
      <summary>
        <time>
          Aug 4, 2020
        </time>
        <div class="title">
          <a href="https://prog-synth-science.github.io/2020/">Program Synthesis for Scientific Computing</a>
        </div>
        <div class="speaker">
          Workshop on using software synthesis for next-generation scientific software
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Jun 24, 2020
        </time>
        <div class="title">
          <a href='talks/fptalks20.html'>FPTalks 2020 annual workshop</a>
        </div>
        <div class="speaker">
          15 speakers at the cutting edge of numerics research
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>
          Jan, 2020
        </time>
        <div class="title">
          <a href="http://fpanalysistools.org/LANL/">FP analysis tools</a>
        </div>
        <div class="speaker">
          A tutorial run at LANL
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>Nov 17, 2019</time>
        <div class="title">
          <a href="http://fpanalysistools.org/sc19/">FP analysis tools</a>
        </div>
        <div class="speaker">
          A tutorial co-located with SC'19
        </div>
      </summary>
    </details>

    <details>
      <summary>
        <time>Jul 30, 2019</time>
        <div class="title">
          <a href="http://fpanalysistools.org/pearc19/">FP analysis tools</a>
        </div>
        <div class="speaker">
          A tutorial co-located with PEARC'19
        </div>
      </summary>
    </details>

    <!--

    <details>
      <summary>
      <time>TIME</time>
      TITLE
      <div class="speaker">
        TODO
      </div>
      </summary>
      <div class="abstract">
        <p>
          TODO
        </p>
      </div>
    </details>

-->

  </main>
</body>

</html>
